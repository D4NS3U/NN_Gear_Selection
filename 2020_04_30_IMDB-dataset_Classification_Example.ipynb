{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPa6OQ0CPLJhQbnOQr2IU/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlyingHirsch96/NN_Gear_Selection/blob/master/2020_04_30_IMDB-dataset_Classification_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "730navdsPqnG",
        "colab_type": "text"
      },
      "source": [
        "#LSTM for classifying the IMDB-dataset to good or bad reviews\n",
        "This code is based of this site: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "The IMDB-dataset is a conglomeration of movie reviews. This LSTM NN tries to classify the movie reviews to the both classes \"good\" and \"bad\" review/movie. One review is a list of numbers, in which every number stands for a single word. The training data consists of a number of reviews, which means, that the training data is a matrix consisting of lists of up to 500 numbers.\n",
        "\n",
        "The NN is trained by inputting one review after another, in which every word resp. number gets inputed after another. The wanted output of the NN exists as an array of 0s and 1s. Therefore the NN gets a sequence (the word by word resp. number by number review of a given movie) as an input and learns to classify it to one of two single outputs.\n",
        "\n",
        "Based on this behaviour, the LSTM behaves as a 'many-to-one' NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4KECGLZeWpJ",
        "colab_type": "code",
        "outputId": "ca95f84b-9f28-4796-de06-61384adc2cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "#fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "#load the IMDB dataset but only keep the top n words, zero the rest\n",
        "topWords = 5000\n",
        "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(num_words=topWords)\n",
        "\n",
        "#truncate and pad input sequences\n",
        "maxReviewLength = 500\n",
        "xTrain = sequence.pad_sequences(xTrain, maxlen=maxReviewLength)\n",
        "xTest = sequence.pad_sequences(xTest, maxlen=maxReviewLength)\n",
        "\n",
        "# create the model\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(topWords, embedding_vector_length, input_length=maxReviewLength))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(xTrain, yTrain, validation_data=(xTest, yTest), epochs=3, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 329s 13ms/step - loss: 0.4804 - accuracy: 0.7670 - val_loss: 0.3763 - val_accuracy: 0.8468\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 323s 13ms/step - loss: 0.3605 - accuracy: 0.8555 - val_loss: 0.6182 - val_accuracy: 0.6361\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 325s 13ms/step - loss: 0.5505 - accuracy: 0.7352 - val_loss: 0.6023 - val_accuracy: 0.6647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7b918fceb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdU7gwkCS8mb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "647cbe1a-4c2a-4d72-dd62-b29060d72a61"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "topWords = 5000\n",
        "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(num_words=topWords)\n",
        "yTrain"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}